{
    "componentName": "LlamaVideoSummarization",
    "componentVersion": "9.0",
    "middlewareVersion": "9.0",
    "sourceLanguage": "python",
    "batchLibrary": "LlamaVideoSummarization",
    "environmentVariables": [],
    "algorithm": {
        "name": "LLAMAVIDEO",
        "description": "LLaMA video summarization",
        "actionType": "DETECTION",
        "trackType": "TEXT",
        "outputChangedCounter": 1,
        "requiresCollection": {
            "states": []
        },
        "providesCollection": {
            "states": [
                "DETECTION",
                "DETECTION_TEXT",
                "DETECTION_TEXT_LLAMA_VIDEO"
            ],
            "properties": [
                {
                    "name": "PROCESS_FPS",
                    "description": "Specifies the number of frames per second of video to process. Between 0 and 10.",
                    "type": "INT",
                    "defaultValue": "1"
                },
                {
                    "name": "MAX_FRAMES",
                    "description": "Specifies the maximum number of frames to process. NOTE: Doesn't seem to work.",
                    "type": "INT",
                    "defaultValue": "180"
                },
                {
                    "name": "MAX_NEW_TOKENS",
                    "description": "Specifies the maximum number of output tokens to generate. Ignores the number of tokens in the input prompt. Increase this value if the response is empty. May need to increase for longer videos.",
                    "type": "INT",
                    "defaultValue": "1024"
                },
                {
                    "name": "GENERATION_PROMPT_PATH",
                    "description": "Path to the text file containing the generation prompt.",
                    "type": "STRING",
                    "defaultValue": "default_prompt.txt"
                },
                {
                    "name": "GENERATION_JSON_SCHEMA_PATH",
                    "description": "Path to the JSON file containing the JSON schema format for the model to generate.",
                    "type": "STRING",
                    "defaultValue": "default_schema.json"
                },
                {
                    "name": "SYSTEM_PROMPT_PATH",
                    "description": "Path to the text file containing the system (role) prompt. If empty, the generation prompt is used for the system prompt.",
                    "type": "STRING",
                    "defaultValue": ""
                },
                {
                    "name": "GENERATION_MAX_ATTEMPTS",
                    "description": "Maximum number of attempts for the model to generate output in the correct JSON format with a timeline that covers the full length of the video.",
                    "type": "INT",
                    "defaultValue": "3"
                },
                {
                    "name": "TIMELINE_CHECK_THRESHOLD",
                    "description": "Specifies the number of seconds the video length determined by the model can differ from the last event end time. If exceeded, another attempt will be made to generate the output. Set to -1 to disable check.",
                    "type": "INT",
                    "defaultValue": "20"
                }
            ] 
        }
    },
    "actions": [
        {
            "name": "LLAMA VIDEO SUMMARIZATION ACTION",
            "description": "Runs LLaMA video summarization.",
            "algorithm": "LLAMAVIDEO",
            "properties": []
        }
    ],
    "tasks": [
        {
            "name": "LLAMA VIDEO SUMMARIZATION TASK",
            "description": "Runs LLaMA video summarization.",
            "actions": [
                "LLAMA VIDEO SUMMARIZATION ACTION"
            ]
        }
    ],
    "pipelines": [
        {
            "name": "LLAMA VIDEO SUMMARIZATION PIPELINE",
            "description": "Runs LLaMA video summarization.",
            "tasks": [
                "LLAMA VIDEO SUMMARIZATION TASK"
            ]
        }
    ]
}