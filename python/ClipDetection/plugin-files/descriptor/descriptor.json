{
    "componentName": "ClipDetection",
    "componentVersion": "9.0",
    "middlewareVersion": "9.0",
    "sourceLanguage": "python",
    "batchLibrary": "ClipDetection",
    "environmentVariables": [],
    "algorithm": {
        "name": "CLIP",
        "description": "CLIP classification.",
        "actionType": "DETECTION",
        "trackType": "CLASS",
        "outputChangedCounter": 1,
        "requiresCollection": {
            "states": []
        },
        "providesCollection": {
            "states": [
                "DETECTION",
                "DETECTION_CLASS",
                "DETECTION_CLASS_CLIP"
            ],
            "properties": [
                {
                    "name": "MODEL_NAME",
                    "description": "Specifies which CLIP model to load for inferencing. The available models are 'ViT-L/14', 'ViT-B/32', and 'CoOp'.",
                    "type": "STRING",
                    "defaultValue": "ViT-L/14"
                },
                {
                    "name": "NUMBER_OF_CLASSIFICATIONS",
                    "description": "The number of classifications, N, to be returned. The N highest confidence classifications found by the network will be returned with their associated confidence values. The value must be greater than 0, and less than the size of the model output layer.",
                    "type": "INT",
                    "defaultValue": "1"
                }, 
                {
                    "name": "TEMPLATE_TYPE",
                    "description": "The number of templates to be used in the text encoder. The current acceptable values are 'openai_1', 'openai_7', and 'openai_80'.",
                    "type": "STRING",
                    "defaultValue": "openai_80"
                }, 
                {
                    "name": "TEMPLATE_PATH",
                    "description": "Optionally specifies a path to a custom text file containing templates for use in the CLIP model. Include a single {} where each classification is to be inserted. If MODEL_NAME=='CoOp', then '' is the only supported value.",
                    "type": "STRING",
                    "defaultValue": ""
                },
                {
                    "name": "CLASSIFICATION_LIST",
                    "description": "Specifies the classification list that will be tokenized for the text encoder (supports 'imagenet' and 'coco'). By default, the COCO classifications will be used. If MODEL_NAME=='CoOp', then 'imagenet' is the only supported value.",
                    "type": "STRING",
                    "defaultValue": "coco"
                },
                {
                    "name": "CLASSIFICATION_PATH",
                    "description": "Optionally specifies a path to a custom csv file containing two names for each classification: one is the full name to display and the other to enter into the CLIP text encoding. If MODEL_NAME=='CoOp', then '' is the only supported value.",
                    "type": "STRING",
                    "defaultValue": ""
                },
                {
                    "name": "ENABLE_CROPPING",
                    "description": "If true, the image will be cropped into 144 images of size 224x224. The results from each of these images is averaged to get the results. Not recommended for use on CPU.",
                    "type": "BOOLEAN",
                    "defaultValue": "true"
                },
                {
                    "name": "ENABLE_BINARY_CLASSIFICATION",
                    "description": "If true, the image will be queried individually for each classification. The results will contain a track for each classification that is positively identified.",
                    "type": "BOOLEAN",
                    "defaultValue": "false"
                },
                {
                    "name": "ENABLE_TRITON",
                    "description": "If true, inferencing will be performed via a configured Triton inference server. If MODEL_NAME=='CoOp', then 'false' is the only supported value.",
                    "type": "BOOLEAN",
                    "defaultValue": "false"
                },
                {
                    "name": "TRITON_SERVER",
                    "description": "Triton server <host>:<port> to use for inferencing.",
                    "type": "STRING",
                    "defaultValue": "clip-detection-server:8001"
                },
                {
                    "name": "INCLUDE_FEATURES",
                    "description": "If true, the detection will have a detection property, FEATURE, which contains the base64-encoded version of the feature vector.",
                    "type": "BOOLEAN",
                    "defaultValue": "false"
                },
                {
                    "name": "DETECTION_FRAME_BATCH_SIZE",
                    "description": "Number of frames to batch inference when processing video. GPU VRAM dependant. If ENABLE_CROPPING is set to true, then the value will be ignored and set to 1.",
                    "type": "INT",
                    "defaultValue": "64"
                },
                {
                    "name": "CUDA_DEVICE_ID",
                    "description": "ID of CUDA device (typically 0) that will be used to run the models. When less than 0 CUDA will be disabled.",
                    "type": "INT",
                    "propertiesKey": "detection.cuda.device.id"
                }
            ] 
        }
    },
    "actions": [
        {
            "name": "CLIP COCO CLASSIFICATION ACTION",
            "description": "Runs CLIP classification on the COCO dataset classes.",
            "algorithm": "CLIP",
            "properties": []
        },
        {
            "name": "CLIP TRITON COCO CLASSIFICATION ACTION",
            "description": "Runs CLIP classification on the COCO dataset classes using a Triton Inferencing Server.",
            "algorithm": "CLIP",
            "properties": [
                {
                    "name": "ENABLE_TRITON",
                    "value": "true"
                },
                {
                    "name": "DETECTION_FRAME_BATCH_SIZE",
                    "value": "32"
                }
            ]
        },
        {
            "name": "CLIP IMAGENET CLASSIFICATION ACTION",
            "description": "Runs CLIP classification on the ImageNet dataset classes.",
            "algorithm": "CLIP",
            "properties": [
                {
                    "name": "CLASSIFICATION_LIST",
                    "value": "imagenet"
                }
            ]
        },
        {
            "name": "CLIP TRITON IMAGENET CLASSIFICATION ACTION",
            "description": "Runs CLIP classification on the ImageNet dataset classes using a Triton Inferencing Server.",
            "algorithm": "CLIP",
            "properties": [
                {
                    "name": "ENABLE_TRITON",
                    "value": "true"
                },
                {
                    "name": "CLASSIFICATION_LIST",
                    "value": "imagenet"
                },
                {
                    "name": "DETECTION_FRAME_BATCH_SIZE",
                    "value": "32"
                }
            ]
        }
    ],
    "tasks": [
        {
            "name": "CLIP COCO CLASSIFICATION TASK",
            "description": "Runs CLIP classification on the COCO dataset classes.",
            "actions": [
                "CLIP COCO CLASSIFICATION ACTION"
            ]
        },
        {
            "name": "CLIP TRITON COCO CLASSIFICATION TASK",
            "description": "Runs CLIP classification on the COCO dataset classes using a Triton Inferencing Server.",
            "actions": [
                "CLIP TRITON COCO CLASSIFICATION ACTION"
            ]
        },
        {
            "name": "CLIP IMAGENET CLASSIFICATION TASK",
            "description": "Runs CLIP classification on the ImageNet dataset classes.",
            "actions": [
                "CLIP IMAGENET CLASSIFICATION ACTION"
            ]
        },
        {
            "name": "CLIP TRITON IMAGENET CLASSIFICATION TASK",
            "description": "Runs CLIP classification on the ImageNet dataset classes using a Triton Inferencing Server.",
            "actions": [
                "CLIP TRITON IMAGENET CLASSIFICATION ACTION"
            ]
        }
    ],
    "pipelines": [
        {
            "name": "CLIP COCO CLASSIFICATION PIPELINE",
            "description": "Runs CLIP classification on the COCO dataset classes.",
            "tasks": [
                "CLIP COCO CLASSIFICATION TASK"
            ]
        },
        {
            "name": "CLIP TRITON COCO CLASSIFICATION PIPELINE",
            "description": "Runs CLIP classification on the COCO dataset classes using a Triton Inferencing Server.",
            "tasks": [
                "CLIP TRITON COCO CLASSIFICATION TASK"
            ]
        },
        {
            "name": "CLIP IMAGENET CLASSIFICATION PIPELINE",
            "description": "Runs CLIP classification on the ImageNet dataset classes.",
            "tasks": [
                "CLIP IMAGENET CLASSIFICATION TASK"
            ]
        },
        {
            "name": "CLIP TRITON IMAGENET CLASSIFICATION PIPELINE",
            "description": "Runs CLIP classification on the ImageNet dataset classes using a Triton Inferencing Server.",
            "tasks": [
                "CLIP TRITON IMAGENET CLASSIFICATION TASK"
            ]
        }
    ]
}